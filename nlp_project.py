# -*- coding: utf-8 -*-
"""nlp-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xKZOvfINvQ-z6hDm4ifAOyPLcA-BXHn0
"""

!pip install polyglot

!pip install pyicu

!pip install morfessor

!pip install pycld2

!polyglot download morph2.sah

!pip install fasttext

!pip install sortedcollections

!pip install pyjarowinkler

!pip install unidecode

"""###Clustering"""

import fasttext
import functools
import argparse
import sys
from collections import defaultdict, Counter
from sortedcollections import ValueSortedDict
from collections import OrderedDict

#from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import homogeneity_completeness_v_measure

from numpy import inner
from numpy.linalg import norm

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn import linear_model
from sklearn.neural_network import MLPClassifier
from ast import literal_eval as make_tuple
from sklearn.metrics import confusion_matrix
import itertools

from sklearn.cluster import AgglomerativeClustering

from pyjarowinkler import distance

import unidecode

import matplotlib
#matplotlib.use('Agg')
import matplotlib.pyplot as plt

import logging

import numpy as np

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.cluster import AgglomerativeClustering

OOV_EMB_SIM = 0.9

def plot_dendrogram(model, **kwargs):


    # Children of hierarchical clustering
    children = model.children_

    # Distances between each pair of children
    # Since we don't have this information, we can use a uniform one for plotting
    distance = np.arange(children.shape[0])

    # The number of observations contained in each cluster level
    no_of_observations = np.arange(2, children.shape[0]+2)

    # Create linkage matrix and then plot the dendrogram
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

    plt.xticks(rotation=90)
    plt.margins(0.2)
    plt.subplots_adjust(bottom=0.2)

def devow(form):
    # implicit transliteration and deaccentization
    uform = unidecode.unidecode(form)

    # keep first letter
    dform = uform[1:]
    # remove vowels, do not presuppose lowercasing
    dform = dform.replace("у", "")
    dform = dform.replace("ё", "")
    dform = dform.replace("е", "")
    dform = dform.replace("ы", "")
    dform = dform.replace("а", "")
    dform = dform.replace("о", "")
    dform = dform.replace("э", "")
    dform = dform.replace("я", "")
    dform = dform.replace("и", "")
    dform = dform.replace("ю", "")
    dform = dform.replace("ө", "")
    dform = dform.replace("ү", "")
    
    return uform[:1] + dform

def embsim(word, otherword):
    if word in embedding and otherword in embedding:
        emb1 = embedding[word]
        emb2 = embedding[otherword]
        sim = inner(emb1, emb2)/(norm(emb1)*norm(emb2))
        # sim = cosine_similarity([emb1], [emb2])
        #logging.debug(sim)
        assert sim >= -1.0001 and sim <= 1.0001, "Cos sim must be between -1 and 1"
        # shift to 0..1 range
        sim = (sim+1)/2
    else:
        # backoff
        sim = OOV_EMB_SIM
    return sim

def jw_safe(srcword, tgtword):
    if srcword == '' or tgtword == '':
        # 1 if both empty
        # 0.5 if one is length 1
        # 0.33 if one is length 2
        # ...
        return 1/(len(srcword)+len(tgtword)+1)
    elif srcword == tgtword:
        return 1
    else:
        return distance.get_jaro_distance(srcword, tgtword)

def jwsim(word, otherword):
    # called distance but is actually similarity
    sim = jw_safe(word, otherword)
    uword = devow(word)
    uotherword = devow(otherword)
    usim = jw_safe(uword, uotherword)    
    sim = (sim+usim)/2
    assert sim >= 0 and sim <= 1, "JW sim must be between 0 and 1"
    return sim

length = 0.5

def lensim(word, otherword):
    global lenght
    return 1 / (1 + length * abs(len(word) - len(otherword)) )

def similarity(word, otherword, similarity):
    if similarity == 'jw':
        return jwsim(word, otherword)
    elif similarity == 'jwxcos':
        return jwsim(word, otherword) * embsim(word, otherword)
    elif similarity == 'jwxcosxlen':
        return jwsim(word, otherword) * embsim(word, otherword) * lensim(word, otherword);
    elif similarity == 'len':
        return lensim(word, otherword);
    else:
        # cos
        return embsim(word, otherword)

# def get_stem(form, remerging=False):
    # if args.lowercase:
    #     form = form.lower()

    # if args.devow:
    #     form = devow(form)

    # if remerging:
    #     stem = form[:args.remerge]
    # else:
    #     stem = form[:args.stems]
    
    # if args.postags:
    #     stem = stem + '_' + postag[form]

    # return stem

remerge = 3

from polyglot.text import Text, Word

def get_stem(form, remerging=False):
  global remerge
  if remerging:
    stem = form[:remerge]
  else:
    form = Word(form, language="sah")
    stem = form.morphemes[0]
  return stem

# logging.info('Read in embeddings')
# if args.embeddings.endswith('.bin'):
#     # get word embedding still the same way, i.e. as embedding[word]
#     # TODO no iterating over this
#     # (or if, then iterate over embedding.words)
#     embedding = fasttext.load_model(args.embeddings)
# else:
#     embedding = defaultdict(list)
#     forms_stemmed = defaultdict(set)
#     form_freq_rank = dict()
#     with open(args.embeddings) as embfile:
#         size, dim = map(int, embfile.readline().split())
#         if args.number:
#             size = min(size, args.number)
#         for i in range(size):
#             fields = embfile.readline().split()
#             form = fields[0]
#             emb = list(map(float, fields[1:]))
#             if args.normalize:
#                 emb /= norm(emb)
#             if args.lowercase and not form.islower():
#                 form = form.lower()
#                 if form in embedding:
#                     # do not overwrite "bush" with "Bush"
#                     continue
#             embedding[form] = emb
#             stem = get_stem(form)
#             forms_stemmed[stem].add(form)
#             form_freq_rank[form] = i

embedding = fasttext.load_model("/content/drive/MyDrive/nlp/cc.sah.300.bin")
forms_stemmed = defaultdict(set)
form_freq_rank = dict()
for i in range(len(embedding.words)):
  stem = get_stem(embedding.words[i])
  forms_stemmed[stem].add(embedding.words[i])
  form_freq_rank[embedding.words[i]] = i

# if args.verbose:
#     for form in sorted(embedding.keys()):
#         logging.debug(form + ' -> ' + get_stem(form))

# logging.info('Read in test form-lemma pairs')
test_data = list()
# with open(args.conllu_test) as conllufile:
#     for line in conllufile:
#         fields = line.split()
#         if fields and fields[0].isdecimal():
#             assert len(fields) > 2
#             form = fields[1]
#             lemma = fields[2]
#             # pos = fields[3]
#             if args.lowercase:
#                 form = form.lower()
#                 lemma = lemma.lower()
#             test_data.append((form, lemma))

import pandas as pd

df = pd.read_csv("/content/drive/My Drive/ddata/df_train.csv")
for i in range(len(df['0'])):
  test_data.append((df['1'][i], df['0'][i]))

from random import shuffle
shuffle(test_data)

def get_dist(form1, form2, sim):
    # similarity to distance
    return 1-similarity(form1, form2, sim)

def node2str(node, index2word):
    return [index2word[index] for index in node]

def linkage(cluster1, cluster2, D, measure):
    linkages = list()
    for node1 in cluster1:
        for node2 in cluster2:
            linkages.append(D[node1, node2])
    # min avg max
    if measure == 'average':
        return sum(linkages)/len(linkages)
    elif measure == 'single':
        return min(linkages)
    elif measure == 'complete':
        return max(linkages)
    else:
        assert False

def cl(stem, cluster):
    return stem + '___' + str(cluster)

threshold = 0.4

def aggclust(forms_stemmed, measure):
    # form -> cluster
    result = dict()
    global threshold
    for stem in forms_stemmed:
        # vocabulary
        index2word = list(forms_stemmed[stem])
        I = len(index2word)
        
        logging.debug(stem)
        logging.debug(I)
        logging.debug(index2word)
        
        if I == 1:
            result[index2word[0]] = cl(stem, 0)
            continue

        D = np.empty((I, I))
        for i1 in range(I):
            for i2 in range(I):
                D[i1,i2] = get_dist(index2word[i1], index2word[i2], 'jwxcos')
        clustering = AgglomerativeClustering(affinity='precomputed',
                linkage = measure, n_clusters=1)
        clustering.fit(D)

        # default: each has own cluster
        clusters = list(range(I))
        nodes = [[i] for i in range(I)]
        for merge in clustering.children_:
            # check stopping criterion
            if threshold < linkage(nodes[merge[0]], nodes[merge[1]], D, 'average'):
                break
            # perform the merge
            nodes.append(nodes[merge[0]] + nodes[merge[1]])
            # reassign words to new cluster ID
            for i in nodes[-1]:
                clusters[i] = len(nodes) - 1
        for i, cluster in enumerate(clusters):
            result[index2word[i]] = cl(stem, cluster)
    return result

# if args.plot:
#        plt.title('Hierarchical Clustering Dendrogram')
#        plot_dendrogram(clustering, labels=index2word)
#        plt.show()

def writeout_clusters(clustering):
    cluster2forms = defaultdict(list)
    for form, cluster in clustering.items():
        cluster2forms[cluster].append(form)
    for cluster in sorted(cluster2forms.keys()):
        print('CLUSTER', cluster)
        for form in cluster2forms[cluster]:
            print(form)
        print()
    sys.stdout.flush()

clusterset = set()

def rename_clusters(clustering):
    cluster2forms = defaultdict(list)
    for form, cluster in clustering.items():
        cluster2forms[cluster].append(form)

    cluster2newname = dict()
    for cluster, forms in cluster2forms.items():
        form2rank = dict()
        for form in forms:
            assert form in form_freq_rank
            form2rank[form] = form_freq_rank[form]
        most_frequent_form = min(form2rank, key=form2rank.get)
        cluster2newname[cluster] = most_frequent_form
        clusterset.add(most_frequent_form)

    new_clustering = dict()
    for form, cluster in clustering.items():
        new_clustering[form] = cluster2newname[cluster]

    return new_clustering

# now 1 nearest neighbour wordform;
# other option is nearest cluster in avg linkage
# (probably similar result but not necesarily)
def find_cluster_for_form(form, clustering, oov):
    global threshold
    stem = get_stem(form)
    cluster = form  # backoff: new cluster
    if oov == "guess" and stem in forms_stemmed:
        dists = dict()
        for otherform in forms_stemmed[stem]:
            dists[otherform] = get_dist(form, otherform, 'jwxcos')
        nearest_form = min(dists, key=dists.get)
        if dists[nearest_form] < threshold:
            cluster = clustering[nearest_form]
            # else leave the default, i.e. a separate new cluster
    return cluster

clusters_restemmed = defaultdict(list)
cluster_remerged = dict()

remergethreshold = 0.3

def remerge(pivot_cluster):
    global remergethreshold
    merged_clusters = set()
    merged_clusters.add(pivot_cluster)
    stem = get_stem(pivot_cluster, remerging=True)
    for candidate_cluster in clusters_restemmed[stem]:
        # find all near clusters; just look at the representant words
        if get_dist(pivot_cluster, candidate_cluster, 'jwxcos') < remergethreshold:
            merged_clusters.add(candidate_cluster)
    # find name for the new merghed cluster
    form2rank = dict()
    form2rank[pivot_cluster] = len(embedding.words)
    for form in merged_clusters:
        if form in form_freq_rank:
            form2rank[form] = form_freq_rank[form]
    merged_name = min(form2rank, key=form2rank.get)
    # define the merge
    print('MERGE:', merged_name, merged_clusters)
    for cluster in merged_clusters:
        cluster_remerged[cluster] = merged_name

def homogeneity(clustering, writeout=False, remerge=False):
    golden = list()
    predictions = list()
    lemmatization_corrects = 0
    found_clusters = dict()  # caching
    lemma2clusters2forms = defaultdict(lambda: defaultdict(set))
    if remerge:
        for cluster in clusterset:
            clusters_restemmed[get_stem(cluster, remerging=True)].append(cluster)
    
    for form, lemma in test_data:
        golden.append(lemma)
        if form in clustering:
            cluster = clustering[form]
        else:
            if form not in found_clusters:
                found_clusters[form] = find_cluster_for_form(form, clustering, 'guess')
            cluster = found_clusters[form]
        if lemma in clustering:
            lemmacluster = clustering[lemma]
        else:
            if lemma not in found_clusters:
                found_clusters[lemma] = find_cluster_for_form(lemma, clustering, 'guess')
            lemmacluster = found_clusters[lemma]

        if remerge:
            if cluster not in cluster_remerged:
                remerge(cluster)
            cluster = cluster_remerged[cluster]
            if lemmacluster not in cluster_remerged:
                remerge(lemmacluster)
            lemmacluster = cluster_remerged[lemmacluster]

        predictions.append(cluster)
        lemma2clusters2forms[lemma][cluster].add(form)
        if cluster == lemmacluster:
            lemmatization_corrects += 1
        if writeout:
            oov = 'OOVform' if form in found_clusters else ''
            lemmaoov = 'OOVlemma' if lemma in found_clusters else ''
            dist = get_dist(form, lemma, 'jwxcos')
            good = 'GOOD' if cluster == lemmacluster else 'BAD'
            print(oov, form, '->', cluster, good,
                    '{:.4f}'.format(dist), lemmaoov, lemma, '->', lemmacluster)
    if writeout:
        print('PER LEMMA WRITEOUT')
        for lemma in lemma2clusters2forms:
            print('LEMMA:', lemma)
            for cluster in lemma2clusters2forms[lemma]:
                print(get_stem(cluster), cluster, ':', lemma2clusters2forms[lemma][cluster])
            print()
    hcv = homogeneity_completeness_v_measure(golden, predictions)
    acc = lemmatization_corrects/len(golden)
    return (*hcv, acc)

def baseline_clustering(test_data, basetype):
    result = dict()
    for form, lemma in test_data:
        for word in (form, lemma):
            stem = get_stem(word)
            if basetype == 'formlemma':
                result[word] = cl(stem, word)
            elif basetype == 'stemlemma':
                result[word] = cl(stem, 0)
            elif basetype == 'upper':
                result[word] = cl(stem, lemma)
            elif basetype == 'stem5':
                result[word] = cl(stem, word[:5])
            logging.debug(basetype + ': ' + word + ' -> ' + result[word])
    return result

known = 0
unknown = 0
for form, _ in test_data:
    if form in embedding:
        known += 1
    else:
        unknown += 1
print('OOV rate:', unknown, '/', (known+unknown), '=',
        (unknown/(known+unknown)*100))

print('Type', 'homogeneity', 'completenss', 'vmeasure', 'accuracy', sep='\t')
for basetype in ('formlemma', 'stemlemma', 'stem5', 'upper'):
    clustering = baseline_clustering(test_data, basetype)
    hcva = homogeneity(clustering)
    print(basetype, *hcva, sep='\t')


clustering = aggclust(forms_stemmed, 'average')
logging.info('Rename clusters')
renamed_clustering = rename_clusters(clustering)
logging.info('Write out train clusters')
print('START TRAIN CLUSTERS')
writeout_clusters(renamed_clustering)
print('END TRAIN CLUSTERS')
logging.info('Run evaluation')
hcva = homogeneity(renamed_clustering, writeout=True)
print('Homogeneity', 'completenss', 'vmeasure', 'accuracy', sep='\t')
print(*hcva, sep='\t')